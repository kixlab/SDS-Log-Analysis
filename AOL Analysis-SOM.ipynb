{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import random\n",
    "plt.close('all')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "queries = pd.read_csv('new_logs.csv', index_col=\"idx\", dtype={\"AnonID\": \"Int64\", \"Query\": \"string\", \"QueryTime\": \"string\", \"ItemRank\": \"Int32\", \"ClickURL\": \"string\", \"Type\": \"string\", \"SessionNum\": \"Int32\"})\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "group_by_sessions = queries.groupby('AnonID', sort=False, as_index=False)\n",
    "qqq = group_by_sessions.max(numeric_only=True)\n",
    "newIDs = qqq.loc[qqq['SessionNum'] >= 19]\n",
    "print(newIDs)\n",
    "# print(newIDs[\"AnonID\"])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "         AnonID  SessionNum  ItemRank\n",
      "0         91602         286       189\n",
      "1       2846780         192        40\n",
      "2       1750999         363       364\n",
      "3        118401         322       119\n",
      "4        186465         159       182\n",
      "...         ...         ...       ...\n",
      "53028  18287420          30      <NA>\n",
      "53643   3962900          23        35\n",
      "54960   1466736          20         1\n",
      "63748   7213879          26         5\n",
      "64443  16419908          25         1\n",
      "\n",
      "[15665 rows x 3 columns]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "def compare_queries(queryA, queryB):\n",
    "  # compute semantic similarities and edit distances to gauge query similarities\n",
    "  return 1\n",
    "\n",
    "def flatten_logs(logs_dataframe):\n",
    "  previous_row = None\n",
    "  previous_query = ''\n",
    "  flattened = []\n",
    "  \n",
    "  for idx, row in logs_dataframe.iterrows():\n",
    "    if previous_row is None: # Every logstream starts with a query\n",
    "      print(row)\n",
    "      flattened.append({\n",
    "        \"type\": \"Query\",\n",
    "        \"query\": row['Query']\n",
    "      })\n",
    "      previous_row = row\n",
    "      previous_query = row['Query']\n",
    "      \n",
    "      continue\n",
    "\n",
    "    if row[\"Type\"] == \"Query\":\n",
    "      if compare_queries(previous_query, row['Query']) >= 1:\n",
    "        flattened.append({\n",
    "          \"type\": \"NewQuery\",\n",
    "          \"query\": row['Query']\n",
    "        })\n",
    "      else:\n",
    "        flattened.append({\n",
    "          \"type\": \"RefinedQuery\",\n",
    "          \"query\": row['Query']\n",
    "        })\n",
    "    elif row[\"Type\"] == \"Click\":\n",
    "      if row[\"ItemRank\"] == 1:\n",
    "        flattened.append({\n",
    "          \"type\": \"Click1\",\n",
    "          \"rank\": row['ItemRank']\n",
    "        })\n",
    "      elif row[\"ItemRank\"] >= 2 and row[\"ItemRank\"] < 6:\n",
    "        flattened.append({\n",
    "          \"type\": \"Click2-5\",\n",
    "          \"rank\": row['ItemRank']\n",
    "        })\n",
    "      elif row[\"ItemRank\"] >= 6 and row[\"ItemRank\"] < 10:\n",
    "        flattened.append({\n",
    "          \"type\": \"Click6-10\",\n",
    "          \"rank\": row['ItemRank']\n",
    "        })\n",
    "      else:\n",
    "        flattened.append({\n",
    "          \"type\": \"Click11+\",\n",
    "          \"rank\": row['ItemRank']\n",
    "        })\n",
    "    elif row[\"Type\"] == \"NextPage\":\n",
    "      flattened.append({\n",
    "        \"type\": \"NextPage\"\n",
    "      })\n",
    "\n",
    "  return flattened\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "group_by_sessions = queries.groupby([\"AnonID\", \"SessionNum\"])\n",
    "\n",
    "tuples = []\n",
    "# for idx, row in newIDs.iterrows():\n",
    "#   tuples += [(row[\"AnonID\"], i) for i in range(row[\"SessionNum\"])]\n",
    "\n",
    "## First, extract all tuples with appropriately long sessions\n",
    "\n",
    "ssss = group_by_sessions.count()\n",
    "ss = ssss[ssss[\"Query\"] >= 5]\n",
    "\n",
    "for idx, _ in ss.iterrows():\n",
    "  tuples += [idx]\n",
    "\n",
    "### Then, draw 5,000 samples\n",
    "\n",
    "SAMPLE_SIZE = 3\n",
    "\n",
    "sample = random.sample(tuples, SAMPLE_SIZE)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Pandas(Index=4027015, AnonID=6212116, Query='-', QueryTime='2006-05-26 23:08:16', Type='Query', SessionNum=15, ItemRank=<NA>, ClickURL=<NA>)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/8k/n5c708p107zcftdwsvs9d3140000gn/T/ipykernel_36592/3542802971.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup_by_sessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflatten_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/8k/n5c708p107zcftdwsvs9d3140000gn/T/ipykernel_36592/3262243884.py\u001b[0m in \u001b[0;36mflatten_logs\u001b[0;34m(logs_dataframe)\u001b[0m\n\u001b[1;32m     13\u001b[0m       flattened.append({\n\u001b[1;32m     14\u001b[0m         \u001b[0;34m\"type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Query\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;34m\"query\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Query'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m       })\n\u001b[1;32m     17\u001b[0m       \u001b[0mprevious_row\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "\n",
    "for s in sample:\n",
    "  g = group_by_sessions.get_group(s)\n",
    "  print(flatten_logs(g))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "AnonID                    6212116\n",
      "Query                           -\n",
      "QueryTime     2006-05-26 23:08:16\n",
      "Type                        Query\n",
      "SessionNum                     15\n",
      "ItemRank                     <NA>\n",
      "ClickURL                     <NA>\n",
      "Name: 4027015, dtype: object\n",
      "[{'type': 'Query', 'query': '-'}, {'type': 'NextPage'}, {'type': 'NewQuery', 'query': 'skynet'}, {'type': 'Click2-5', 'rank': 3}, {'type': 'Click2-5', 'rank': 3}, {'type': 'Click2-5', 'rank': 3}, {'type': 'NewQuery', 'query': '-'}, {'type': 'NewQuery', 'query': 'skynet'}, {'type': 'Click2-5', 'rank': 3}, {'type': 'NewQuery', 'query': '-'}, {'type': 'NextPage'}, {'type': 'NewQuery', 'query': 'skynet'}, {'type': 'Click2-5', 'rank': 3}, {'type': 'NewQuery', 'query': '-'}, {'type': 'NewQuery', 'query': 'skynet'}, {'type': 'Click2-5', 'rank': 3}, {'type': 'NewQuery', 'query': '-'}, {'type': 'NextPage'}, {'type': 'NextPage'}, {'type': 'NextPage'}, {'type': 'NextPage'}]\n",
      "AnonID                   14943896\n",
      "Query               cheap tickets\n",
      "QueryTime     2006-04-11 20:18:39\n",
      "Type                        Query\n",
      "SessionNum                     12\n",
      "ItemRank                     <NA>\n",
      "ClickURL                     <NA>\n",
      "Name: 3520777, dtype: object\n",
      "[{'type': 'Query', 'query': 'cheap tickets'}, {'type': 'NewQuery', 'query': '-'}, {'type': 'NewQuery', 'query': 'cheap tickets'}, {'type': 'NewQuery', 'query': 'airtran'}, {'type': 'Click1', 'rank': 1}]\n",
      "AnonID                    5662324\n",
      "Query                 foxtons.com\n",
      "QueryTime     2006-03-12 08:53:32\n",
      "Type                        Query\n",
      "SessionNum                     11\n",
      "ItemRank                     <NA>\n",
      "ClickURL                     <NA>\n",
      "Name: 2565867, dtype: object\n",
      "[{'type': 'Query', 'query': 'foxtons.com'}, {'type': 'Click1', 'rank': 1}, {'type': 'NewQuery', 'query': 'bargainnetworkk.com'}, {'type': 'NextPage'}, {'type': 'NewQuery', 'query': 'bargainnetwork.com'}, {'type': 'NewQuery', 'query': 'movies for kids to watch now'}, {'type': 'Click2-5', 'rank': 4}, {'type': 'NextPage'}, {'type': 'Click11+', 'rank': 17}, {'type': 'NewQuery', 'query': 'disney channel.com'}, {'type': 'Click2-5', 'rank': 2}]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# https://lovit.github.io/visualization/2019/12/02/som_part1/\n",
    "\n",
    "def initialize_simple(n_rows, n_cols):\n",
    "\n",
    "  grid, pairs = make_grid_and_neighbors(n_rows, n_cols)\n",
    "\n",
    "  x_ranges = np.linspace(0, 1, n_rows)\n",
    "  y_ranges = np.linspace(0, 1, n_cols)\n",
    "\n",
    "  C = np.asarray([[x, y] for x in x_ranges for y in y_ranges])\n",
    "\n",
    "  return grid, C, pairs\n",
    "\n",
    "def initialize_markov(n_rows, n_cols, states, initial_probabilities):\n",
    "\n",
    "  grid, pairs = make_grid_and_neighbors(n_rows, n_cols)\n",
    "\n",
    "  x_ranges = np.linspace(0, 1, n_rows)\n",
    "  y_ranges = np.linspace(0, 1, n_cols)\n",
    "\n",
    "  C = np.asarray([MarkovModel(states, None, initial_probabilities) for x in x_ranges for y in y_ranges])\n",
    "\n",
    "\n",
    "  return grid, C, pairs\n",
    "\n",
    "\n",
    "def make_grid_and_neighbors(n_rows, n_cols):\n",
    "  grid = np.arange(n_rows * n_cols).reshape(n_rows, n_cols)\n",
    "  pairs = []\n",
    "  for i in range(n_rows):\n",
    "    for j in range(n_cols):\n",
    "      idx = grid[i, j]\n",
    "      neighbors = []\n",
    "      if j > 0:\n",
    "        neighbors.append(grid[i, j-1])\n",
    "      if i > 0:\n",
    "        neighbors.append(grid[i-1, j])\n",
    "      for nidx in neighbors:\n",
    "        pairs.append((idx, nidx))\n",
    "\n",
    "  return grid, pairs\n",
    "\n",
    "def make_masks(grid, sigma = 1.0, max_width = 2):\n",
    "  rows, cols = np.where(grid >= 0)\n",
    "  data = grid[rows, cols]\n",
    "\n",
    "  sorted_indices = data.argsort()\n",
    "  indices = zip(rows[sorted_indices], cols[sorted_indices])\n",
    "  masks = [make_gaussian_mask(grid, i, j, sigma, max_width) for i, j in indices]\n",
    "  masks = [masks.flatten() for mask in masks]\n",
    "\n",
    "  return masks\n",
    "\n",
    "def make_gaussian_mask(grid, i, j, sigma = 1.0, max_width = 2):\n",
    "  mask = np.zeros(grid.shape)\n",
    "  for i_, j_ in zip(*np.where(grid >= 0)):\n",
    "    if (max_width > 0) and (abs(i - i_) + abs(j - j_) > max_width):\n",
    "      continue\n",
    "    mask[i_, j_] = np.exp(-((i-i_)**2 + (j-j_) ** 2) / sigma ** 2)\n",
    "\n",
    "  return mask\n",
    "\n",
    "def make_neighbor_graph(grid, max_width = 2, decay = 0.25):\n",
    "  def weight_array(f, s):\n",
    "    return np.asarray([np.power(f, i) for i in range(1, s+1) for _ in range(4*i)])\n",
    "\n",
    "  def pertubate(s):\n",
    "    def unique(i, s):\n",
    "      if abs(i) == s:\n",
    "        return [0]\n",
    "      return [s - abs(i), -s + abs(i)]\n",
    "\n",
    "    def pertubate_(s_):\n",
    "      return [(i, j) for i in range(-s, s+1) for j in unique(i, s_)]\n",
    "\n",
    "    return [pair for s_ in range(1, s+1) for pair in pertubate_(s_)]\n",
    "\n",
    "  def is_outbound(i_, j_):\n",
    "    return (i_ < 0) or (i_ >= n_rows) or (j_ < 0) or (j_ >= n_cols)\n",
    "\n",
    "  n_rows, n_cols = grid.shape\n",
    "  n_codes = n_rows * n_cols\n",
    "\n",
    "  W = weight_array(decay, max_width)\n",
    "  N = -np.ones((n_codes, W.shape[0]), dtype = np.int)\n",
    "  N_inv = -np.ones((n_codes, W.shape[0]), dtype = np.int)\n",
    "\n",
    "  for row, (i, j) in enumerate(zip(*np.where(grid >= 0))):\n",
    "    idx_b = grid[i, j]\n",
    "    for col, (ip, jp) in enumerate(pertubate(max_width)):\n",
    "      if is_outbound(i+ip, j+jp):\n",
    "        continue\n",
    "      idx_n = grid[i+ip, j+jp]\n",
    "      N[idx_b, col] = idx_n\n",
    "      N_inv[idx_n, col] = idx_b\n",
    "  \n",
    "  return N, N_inv, W\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "def closest(X, C, metric):\n",
    "  # return (idx, dist)\n",
    "\n",
    "  return pairwise_distances_argmin_min(X, C, metric = metric)\n",
    "\n",
    "def update_stochastic(X, C, lr = 0.01, metric = 'euclidean', masks = None):\n",
    "  n_data = X.shape[0]\n",
    "  n_codes, n_features = C.shape\n",
    "  C_new = C.copy()\n",
    "\n",
    "  Xr = X[np.random.permutation(n_data)]\n",
    "\n",
    "  for i, Xi in enumerate(Xr):\n",
    "    bmu, _ = closest(Xi.reshape(1, -1), C_new, metric)\n",
    "    bmu = int(bmu)\n",
    "\n",
    "    diff = Xi - C_new\n",
    "    grad = lr * diff * masks[bmu][:, np.newaxis]\n",
    "    C_new += grad\n",
    "  \n",
    "  return C_new\n",
    "    \n",
    "\n",
    "def update_cmeans(X, C, update_ratio, metric='euclidean', batch_size = -1, grid = None, neighbors = None, inv_neighbors = None, weights = None, adjust_ratio = 0.5, max_width = 2, decay = 0.25, **kargs):\n",
    "  if (neighbors in None) or (weights is None):\n",
    "    neighbors, inv_neighbors, weights = make_neighbor_graph(grid, max_width, decay)\n",
    "\n",
    "  C_new = C.copy()\n",
    "\n",
    "  for b, Xb in enumerate(to_minibatch(X, batch_size)):\n",
    "    C_new = update_cmeans_batch(Xb, C_new, update_ratio, metric, neighbors, inv_neighbors, weights, adjust_ratio)\n",
    "\n",
    "  return C_new\n",
    "\n",
    "def update_cmeans_batch(X, C, update_ratio, metric, neighbors, inv_neighbors, weights, adjust_ratio):\n",
    "  n_data = X.shape[0]\n",
    "  n_codes = C.shape[0]\n",
    "\n",
    "  C_cont = np.zeros(shape = C.shape)\n",
    "  W_new = np.zeros(n_codes)\n",
    "\n",
    "  bmu, dist = closest(X, C, metric)\n",
    "\n",
    "  for bmu_c in range(n_codes):\n",
    "    indices = np.where(bmu == bmu_c)[0]\n",
    "    n_matched = indices.shape[0]\n",
    "\n",
    "    if n_matched == 0:\n",
    "      continue\n",
    "      \n",
    "    Xc = np.asarray(X[indices, :].sum(axis=0)).reshape(-1)\n",
    "    C_cont[bmu_c] += Xc\n",
    "    W_new[bmu_c] += n_matched\n",
    "\n",
    "    if weights.shape[0] == 0:\n",
    "      continue\n",
    "\n",
    "    for c, w in zip(neighbors[bmu_c], weights):\n",
    "      if c == -1:\n",
    "        continue\n",
    "      C_cont[c] += w * Xc\n",
    "      W_new[c] += w * n_matched\n",
    "\n",
    "  C_new = update_ratio * C_cont + (1 - update_ratio) * C\n",
    "  return C_new\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Markov model specific implementations here\n",
    "\n",
    "def find_pair_from_sequence(seq, state_prev, state_next):\n",
    "  for i in range(len(seq) - 1):\n",
    "    if seq[i]['type'] == state_prev and seq[i+1]['type'] == state_next:\n",
    "      return 1\n",
    "    \n",
    "  return 0\n",
    "\n",
    "def update_markov(sequences, C, lr = 0.01, states = [], metric = 'euclidean', masks = None, grid = None):\n",
    "\n",
    "  K = C.size # grid size\n",
    "  C_flat = C.ravel()\n",
    "  N = len(sequences) # number of inputs\n",
    "  m = len(states)\n",
    "\n",
    "\n",
    "  masks = make_masks(grid, sigma = 1, max_width = 2) # h-matrix, with row vectors as mask for each grid point\n",
    "  delta_matrix = np.asarray([[1 if sequences[_n][1]['type'] == states[_m] else 0 for _m in range(m)] for _n in range(N)]) # m X N deltas\n",
    "  beta_matrix = np.asarray([[[find_pair_from_sequence(sequences[_n], states[_prev], states[_next]) for _next in range(m)] for _prev in range(m)] for _n in range(N)]) # m X m X N matrix of betas\n",
    "\n",
    "  # make_neighbor_graph(grid, max_width =  )\n",
    "\n",
    "  # E-step: compute Q function\n",
    "\n",
    "  p = np.asarray([[C[k].compute_probability(sequences[n]) for k in range(K)] for n in range(N)]) # N by K matrix with probs\n",
    "  \n",
    "  p_c = np.ones((N, K))\n",
    "\n",
    "  for n in range(N):\n",
    "    for k in range(K):\n",
    "      i, j = np.where(grid = n)\n",
    "      mask = masks[k].reshape(C.shape)\n",
    "\n",
    "      for idx, (i, j) in enumerate(zip(*np.where(mask > 0))):\n",
    "        r = grid[i, j]\n",
    "        p_c[n, k] = p_c[n, k] * (p[n, r] ** mask[i, j])\n",
    "\n",
    "\n",
    "  # M-step: update params\n",
    "\n",
    "  # initial states\n",
    "\n",
    "  for k in range(K):\n",
    "    denom_matrix = delta_matrix @ p_c[:, k] @ masks[k] # ending up with (m X N) (N X 1) (1 X K) == m X K \n",
    "    denom_vector = np.sum(denom_matrix, axis = 1)\n",
    "    new_initials = denom_vector / np.sum(denom_vector)\n",
    "    C_flat[k].initial_probabilities = new_initials\n",
    "\n",
    "  # Transition states\n",
    "\n",
    "  for k in range(K):\n",
    "    denom_ndarray = beta_matrix @ p_c[:, k] @ masks[k] # ending up with (m, m, N) (N, 1) (1, K) == m, m, K\n",
    "    denom_mat = np.sum(denom_ndarray, axis = 2)\n",
    "    new_transitions = denom_mat / np.sum(denom_mat, axis = 1)[:, None]\n",
    "\n",
    "    C_flat[k].probabilities = new_transitions\n",
    "\n",
    "    \n",
    "\n",
    "      \n",
    "\n",
    "  \n",
    "\n",
    "  \n",
    "\n",
    "  \n",
    "\n",
    "  \n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1189074415.py, line 12)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/8k/n5c708p107zcftdwsvs9d3140000gn/T/ipykernel_45193/1189074415.py\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    for\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Markov Model\n",
    "\n",
    "class MarkovModel(object):\n",
    "  states = np.ndarray(0)\n",
    "  num_states = 0\n",
    "  probabilities = None\n",
    "  initial_probabilities = np.ndarray(0)\n",
    "\n",
    "  def __init__(self, states, probabilities = None, initial_probabilities = None):\n",
    "    self.states = states\n",
    "    self.probabilities = probabilities\n",
    "    self.initial_probabilities = initial_probabilities\n",
    "\n",
    "\n",
    "    if self.probabilities is None:\n",
    "      self.probabilities = np.ones((len(states), len(states))) / (len(states) ** 2)\n",
    "\n",
    "  def update_probabilities(self, probabilities):\n",
    "    self.probabilities = probabilities\n",
    "\n",
    "  def compute_probability(self, sequence):\n",
    "    prob = 1\n",
    "    prev_state_idx = -1\n",
    "    for idx, state in enumerate(sequence):\n",
    "      cur_state_idx = self.states.where(state['type'])\n",
    "      if idx == 0: # initial prob\n",
    "        prob = prob * self.initial_probabilities[cur_state_idx]\n",
    "        prev_state_idx = cur_state_idx\n",
    "      else:\n",
    "        prob = prob * self.probabilities[prev_state_idx, cur_state_idx]\n",
    "        prev_state_idx = cur_state_idx\n",
    "\n",
    "    \n",
    "    return prob\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.4 64-bit ('miniforge3-4.10.1-3': pyenv)"
  },
  "interpreter": {
   "hash": "73c886d01df8b87bf059d4314531db4f6242e84ca2fadebd6ef97a31f6da79ee"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}